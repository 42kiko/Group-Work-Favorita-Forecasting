from pathlib import Path

import pandas as pd

DATA = {
    "items": "items.parquet",
    "oil": "oil.parquet",
    "holidays_events": "holidays_events.parquet",
    "stores": "stores.parquet",
    "test": "test.parquet",
    "transactions": "transactions.parquet",
    "train": "train.parquet",
}


def load_train_csv(path: str | Path) -> pd.DataFrame:
    """
    LÃ¤dt die Trainingsdaten mit optimierten Typen:
    'boolean' (mit groÃŸem B) erlaubt 0, 1 und NA Werte.
    """
    return pd.read_csv(
        path,
        parse_dates=["date"],
        dtype={
            "id": "int64",
            "item_nbr": "int32",
            "store_nbr": "int16",
            "family": "string",
            "onpromotion": "boolean",
        },
    )


def load_df(path: str | Path) -> pd.DataFrame:
    """Standard-Loader fÃ¼r kleinere Dataframes."""
    return pd.read_csv(path)


def df_to_parquet(df: pd.DataFrame, parquet_path: str | Path) -> None:
    """Speichert einen Dataframe als Parquet-Datei."""
    df.to_parquet(parquet_path, index=False)


def df_to_parquet_packages(df: pd.DataFrame, parquet_path: str | Path) -> None:
    """Speichert einen Dataframe als Parquet-Datei."""

    df.to_parquet(parquet_path, index=False)


def split_parquet_to_packages(
    df: pd.DataFrame, name: str, target_root: str | Path
) -> None:
    """
    Splittet df in Parquet Parts.
    Ziel: mÃ¶glichst nah an target_mb, garantiert nicht grÃ¶ÃŸer als hard_limit_mb.
    """
    target_dir = Path(target_root) / name
    target_dir.mkdir(parents=True, exist_ok=True)

    total_rows = len(df)
    if total_rows == 0:
        print(f"ðŸ“¦ Keine Daten fÃ¼r: {name}")
        return

    target_bytes = int(99.0 * 1024 * 1024)
    hard_limit_bytes = int(99.0 * 1024 * 1024)
    s_rows = min(200_000, total_rows)
    s_start = max(0, (total_rows // 2) - (s_rows // 2))
    tmp = target_dir / "_sample_tmp.parquet"
    df.iloc[s_start : s_start + s_rows].to_parquet(tmp, index=False)
    bytes_per_row = max(1.0, tmp.stat().st_size / s_rows)
    tmp.unlink()
    rows_est = max(1, int(int(99.0 * 1024 * 1024) / bytes_per_row))

    start = 0
    part = 0
    while start < total_rows:
        end_guess = min(total_rows, start + rows_est)
        out_path = target_dir / f"part_{part:01d}.parquet"
        end_final, written_bytes = _write_part_with_hard_limit(
            df=df,
            start=start,
            end=end_guess,
            out_path=out_path,
            hard_limit_bytes=hard_limit_bytes,
        )

        actual_mb = written_bytes / (1024**2)
        print(f"   âœ… {name} Part {part}: {actual_mb:.2f} MB")
        if written_bytes > 0:
            ratio = target_bytes / written_bytes
            rows_est = max(1, int((end_final - start) * ratio))
        start = end_final
        part += 1

    del df
    print(f"ðŸ“¦ Pakete erfolgreich erstellt in: {target_dir}")


def _write_part_with_hard_limit(
    df: pd.DataFrame,
    start: int,
    end: int,
    out_path: Path,
    *,
    hard_limit_bytes: int,
) -> tuple[int, int]:
    """
    Schreibt df[start:end] nach out_path.
    Wenn die Datei grÃ¶ÃŸer als hard_limit_bytes ist, verkleinert end iterativ und schreibt neu.
    """
    end = max(start + 1, end)

    while True:
        df.iloc[start:end].to_parquet(out_path, index=False)
        size = out_path.stat().st_size

        if size <= hard_limit_bytes:
            return end, size

        rows = end - start
        if rows <= 1:
            return end, size

        # verkleinern mit Sicherheitsfaktor
        shrink_ratio = (hard_limit_bytes / size) * 0.98
        new_rows = max(1, int(rows * shrink_ratio))
        end = start + new_rows


def save_tables_to_parquet() -> None:
    """
    Geht die Liste der Tabellen durch und speichert sie als Parquet-Pakete.
    Train landet in ../data/train/, andere in ../data/name_pkg/.
    """

    for element in DATA:
        if element == "train":
            split_parquet_to_packages(
                load_train_csv(f"data/raw/{element}.csv"),
                "train",
                f"data/processed/{element}.parquet",
            )
        else:
            df_to_parquet(
                load_df(f"data/raw/{element}.csv"), f"data/processed/{element}.parquet"
            )


def parquet_loader(name: str) -> pd.DataFrame:
    if name not in DATA:
        raise ValueError(f"{name} ist kein gÃ¼ltiger Datensatz")

    # ðŸ”¹ Spezialfall: train besteht aus mehreren Parts
    if name == "train":
        base_dir = Path("data/processed") / DATA[name] / "train"

        if not base_dir.exists():
            raise FileNotFoundError(f"Train-Verzeichnis nicht gefunden: {base_dir}")

        parts = sorted(base_dir.glob("part_*.parquet"))

        if not parts:
            raise FileNotFoundError(f"Keine Train-Parquet-Parts gefunden in {base_dir}")

        dfs = [pd.read_parquet(p) for p in parts]
        return pd.concat(dfs, ignore_index=True)

    # ðŸ”¹ Standardfall: einzelne Parquet-Datei
    return pd.read_parquet(Path("data/processed") / DATA[name])


# return pd.read_parquet(f"data/processed/{name}.parquet")

# print(parquet_loader("oil"))
